#4A_Tokenization using Python’s split() function 
sent = "Hello, Welcome to python pool, hope you are doing well"
tokens = sent.split()
print(tokens)

#4b_Tokenization using Regular Expressions (RegEx) 
import re
string = "Welcome! to monday night raw."
token = re.findall(r'\w+',string)
print(token)

#4c_Tokenization using NLTK
import nltk
nltk.download('punkt')
from nltk.tokenize import word_tokenize
# Create a string input
str = "I love to study Natural Language Processing in Python"
# Use tokenize method
print(word_tokenize(str))

#4d_ Tokenization using the spaCy library
import spacy
nlp = spacy.blank("en")
print(nlp)
# Create a string input
str = "Harry potter was a highly unusual boy in many ways."
# Create an instance of document;
# doc object is a container for a sequence of Token objects.
doc = nlp(str)
# Read the words; Print the words
words = [word.text for word in doc]
print(words)

#4e_Tokenization using Keras 
from keras.preprocessing.text import text_to_word_sequence
# define the document
text = 'Amit and Jignesh are brothers.'
# tokenize the document
result = text_to_word_sequence(text)
print(result)

#4f_Tokenization using Gensim 
from gensim.utils import tokenize
# Create a string input
str = "No wizarding household is complete without a copy."
# tokenizing the text
list(tokenize(str))

