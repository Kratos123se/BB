#2A_Study of various Corpus – Brown, Inaugural, Reuters,udhr with various methods like fields, raw, words,sents, categories.

# Brown corpus
from nltk.corpus import brown
import nltk
nltk.download('brown')
brown.categories()
brown.words(categories='news')
brown.words(fileids=['cg22'])
brown.sents(categories=['news', 'editorial', 'reviews'])
# reuters corpus
from nltk.corpus import reuters
import nltk
nltk.download('reuters')
reuters.fileids()
reuters.categories()
reuters.categories('training/9865')
reuters.fileids('barley')
reuters.words('training/9865')[:14]
# inaugral corpus
from nltk.corpus import inaugural
import nltk
nltk.download('inaugural')
inaugural.fileids()
[fileid[:4] for fileid in inaugural.fileids()]

#2B_Create and use your own corpora(plaintext,
categorical).
import nltk
nltk.download('punkt')
from nltk.corpus import PlaintextCorpusReader
corpus_root = '/content/sample_data'
filelist = PlaintextCorpusReader(corpus_root, '.*')
print ('\n File list: \n')
print (filelist.fileids())
print (filelist.root)
print ('\n\nStatistics for each text:\n')
print ('AvgWordLen\tAvgSentenceLen\tno.ofTimesEachWordAppearsOnAvg\tFileName')
for fileid in filelist.fileids():
  num_chars = len(filelist.raw(fileid))
  num_words = len(filelist.words(fileid))
  num_sents = len(filelist.sents(fileid))
  num_vocab = len(set([w.lower() for w in filelist.words(fileid)]))
print(int(num_chars/num_words),'\t\t\t', int(num_words/num_sents),'\t\t\t',
int(num_words/num_vocab),'\t\t', fileid)

#2C_Study Conditional frequency distributions.
Study of tagged corpora with methods like
tagged_sents, tagged_words.
text = ['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]
pairs = [('news', 'The'), ('news', 'Fulton'), ('news', 'County'), ...]
import nltk
nltk.download('udhr')
from nltk.corpus import brown
fd = nltk.ConditionalFreqDist(
(genre, word)
for genre in brown.categories()
for word in brown.words(categories=genre))
genre_word = [(genre, word)
for genre in ['news', 'romance']
for word in brown.words(categories=genre)]
print(len(genre_word))
print(genre_word[:4])
print(genre_word[-4:])
cfd = nltk.ConditionalFreqDist(genre_word)
print(cfd)
print(cfd.conditions())
print(cfd['news'])
print(cfd['romance'])
print(list(cfd['romance']))
from nltk.corpus import inaugural
cfd = nltk.ConditionalFreqDist(
(target, fileid[:4])
for fileid in inaugural.fileids()
for w in inaugural.words(fileid)
for target in ['america', 'citizen']
if w.lower().startswith(target))
from nltk.corpus import udhr
languages = ['Chickasaw', 'English', 'German_Deutsch',
'Greenlandic_Inuktikut', 'Hungarian_Magyar', 'Ibibio_Efik']
cfd = nltk.ConditionalFreqDist(
(lang, len(word))
for lang in languages
for word in udhr.words(lang + '-Latin1'))
cfd.tabulate(conditions=['English', 'German_Deutsch'],
samples=range(10), cumulative=True)
# study of tagged corpora
import nltk
brown_lrnd_tagged = brown.tagged_words(categories='learned')
tags = [b[1] for (a, b) in nltk.bigrams(brown_lrnd_tagged) if a[0] == 'often']
fd = nltk.FreqDist(tags)
fd.tabulate()
from nltk.corpus import brown
def process(sentence):
  for (w1,t1), (w2,t2), (w3,t3) in nltk.trigrams(sentence):
    if (t1.startswith('V') and t2 == 'TO' and t3.startswith('V')):
      print(w1, w2, w3)
for tagged_sent in brown.tagged_sents():
  process(tagged_sent)

#2D_Write a program to find the most frequent noun tags
import nltk
nltk.download('all')
def findtags(tag_prefix, tagged_text):
  cfd = nltk.ConditionalFreqDist((tag, word) for (word, tag) in tagged_text
  if tag.startswith(tag_prefix))
  return dict((tag, list(cfd[tag].keys())[:5]) for tag in cfd.conditions())
tagdict = findtags('NN', nltk.corpus.brown.tagged_words(categories='news'))
for tag in sorted(tagdict):
  print(tag, tagdict[tag])

#2E_Map Words to Properties Using Python Dictionaries. 
pos = {}
pos['colorless'] = 'ADJ'
pos['ideas'] = 'N'
pos['sleep'] = 'V'
pos['furiously'] = 'ADV'
print(pos)
pos['ideas']
pos['colorless']
list(pos)
sorted(pos)
[w for w in pos if w.endswith('s')]
for word in sorted(pos):
  print(word + ":", pos[word])
print(pos.keys())
print(pos.values())
print(pos.items())

#2F_Study DefaultTagger, Regular expression tagger,UnigramTagger.
# default tagger
import nltk
from nltk.tag import DefaultTagger
raw = 'I do like to go to gym everyday!'
tokens = nltk.word_tokenize(raw)
default_tagger = nltk.DefaultTagger('NN')
default_tagger.tag(tokens)
#regular expression tagger
from nltk.corpus import brown
from nltk.tag import RegexpTagger
test_sent = brown.sents(categories='news')[0]
patterns = [
(r'.*ing$', 'VBG'), # gerunds
(r'.*ed$', 'VBD'), # simple past
(r'.*es$', 'VBZ'), # 3rd singular present
(r'.*ould$', 'MD'), # modals
(r'.*\'s$', 'NN$'), # possessive nouns
(r'.*s$', 'NNS'), # plural nouns
(r'^-?[0-9]+(.[0-9]+)?$', 'CD'), # cardinal numbers
(r'.*', 'NN') # nouns (default)
]
regexp_tagger = nltk.RegexpTagger(patterns)
print(regexp_tagger.tag(test_sent))
# Unigram tagger
from nltk.corpus import brown
brown_tagged_sents = brown.tagged_sents(categories='news')
brown_sents = brown.sents(categories='news')
unigram_tagger = nltk.UnigramTagger(brown_tagged_sents)
unigram_tagger.tag(brown_sents[2007])

#2G_Find different words from a given plain text withoutany space by comparing this text with a given corpus of words. Also find the score of words.
def word_count(str):
  counts = dict()
  str = str.lower()
  words = str.split()
  for word in words:
    if word in counts:
      counts[word] += 1
    else:
      counts[word] = 1
  return counts
print(word_count('Shree Ram, the Hindu god, is the seventh avatar of the god Vishnu.'))

